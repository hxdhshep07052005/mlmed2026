\documentclass[a4paper,12pt]{report}
\usepackage[margin=1in]{geometry}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{array, booktabs, multirow, longtable}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{makecell}
\usepackage[table]{xcolor}
\usepackage{placeins}
\usepackage{float}
\usepackage{tocloft}
\usepackage{enumitem}
\usepackage[colorlinks=true, linkcolor=black, citecolor=black, urlcolor=black]{hyperref}
\usepackage{cite}
\usepackage{url}
\usepackage{multicol}
\usepackage[activate={true,nocompat},final,tracking=true,kerning=true,spacing=true]{microtype}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\usepackage{inconsolata}

\definecolor{background}{RGB}{250,250,250}
\definecolor{keyword}{RGB}{170,0,255}
\definecolor{importcolor}{RGB}{147,0,255}
\definecolor{string}{RGB}{163,21,21}
\definecolor{comment}{RGB}{0,128,0}
\definecolor{numbercolor}{RGB}{0,0,255}

\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\thesubsection.\arabic{subsubsection}}

\titleformat{\section}
  {\normalfont\Large\bfseries\raggedright}{\thesection}{1em}{}
\titlespacing*{\section}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}

\titleformat{\subsection}
  {\normalfont\large\bfseries\raggedright}{\thesubsection}{1em}{}
\titlespacing*{\subsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\raggedright}{\thesubsubsection}{1em}{}
\titlespacing*{\subsubsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\setlength{\parindent}{0em}
\setlength{\parskip}{0.3em}
\linespread{1.0}

\setlist[itemize]{
  label=--,
  leftmargin=1.5em,
  itemindent=0em,
  itemsep=0.2em,
  parsep=0.1em,
  topsep=0.3em,
  partopsep=0.1em
}

\setlist[enumerate]{
  leftmargin=1.5em,
  itemindent=0em,
  itemsep=0.2em,
  parsep=0.1em,
  topsep=0.3em,
  partopsep=0.1em
}



\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{background},
    basicstyle=\ttfamily\small,
    frame=single,
    showstringspaces=false,
    numbers=none,
    tabsize=4,
    breaklines=true,
    keywordstyle=\color{importcolor}\bfseries,
    commentstyle=\color{comment},
    stringstyle=\color{string},
    emph={from,import,as,for,while,if,else,elif,return,def,class,print},
    emphstyle=\color{keyword}\bfseries,
    moredelim=[s][\color{string}]{'}{'},
    moredelim=[s][\color{string}]{"}{"},
}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,  
    showlines=false,
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle, showlines=false}

\begin{document}

\onecolumn

\thispagestyle{empty}

\begin{tikzpicture}[remember picture, overlay]
    \draw[line width=2pt]
        ($(current page.south west) + (0.5in, 0.5in)$) 
        rectangle 
        ($(current page.north east) - (0.5in, 0.5in)$);
\end{tikzpicture}

\begin{center}
    
    \makebox[\textwidth][s]{\large \textbf{UNIVERSITY OF SCIENCE AND TECHNOLOGY OF HANOI}} \\[0.75cm]
    \makebox[\textwidth][s]{\small \textbf{DEPARTMENT OF INFORMATION AND COMMUNICATION TECHNOLOGY}}
    \vspace{0.5cm}
    
    \includegraphics[width=0.65\textwidth]{image/usth.png}
    \vspace{0.5cm}
    
    {\LARGE \textbf{Labwork 1}}\\[0.5cm]
    \vspace{0.5cm}

    \large
    \begin{tabular}{l c r}
    \\
    \textbf{Dang Dinh Hoa} & 23BI14169 & \\
    \end{tabular}
    \vspace{0.5cm}

    {\Large \textbf{Title:}}\\[0.5cm]
    {\large \textbf{ECG Heartbeat Classification: A Deep Transferable Representation}}\\[0.5cm]
\end{center}

\vspace{0.5cm}

\begin{center}
    \textit{Hanoi, January 2026}
\end{center}

\newpage
\thispagestyle{plain}
\begin{multicols}{2}

\section{Introduction}

This project builds a deep learning model to classify ECG heartbeats into 5 types. The MIT-BIH Arrhythmia dataset is used and the results are compared with the original paper.

\section{Dataset Description}

\subsection{Dataset Overview}

The MIT-BIH Arrhythmia dataset from Kaggle is used. Each heartbeat sample has 187 timesteps (ECG signal values). The dataset is split into:
\begin{itemize}
    \item Training: 87,554 samples
    \item Test: 21,892 samples
\end{itemize}

Figure~\ref{fig:ecg_signal} shows example ECG signals from different classes.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\columnwidth]{image/ECG_signal.png}
\caption{Example ECG signals from different heartbeat classes}
\label{fig:ecg_signal}
\end{figure}

\subsection{Class Distribution}

The dataset has 5 classes following AAMI EC57 standard:
\begin{itemize}
    \item Class 0 (N): 72,471 samples (82.77\%) - Normal
    \item Class 1 (S): 2,223 samples (2.54\%) - Supraventricular
    \item Class 2 (V): 5,781 samples (6.60\%) - Ventricular
    \item Class 3 (F): 641 samples (0.73\%) - Fusion
    \item Class 4 (Q): 6,431 samples (7.35\%) - Unknown
\end{itemize}

The dataset is very imbalanced - Class 0 has 113 times more samples than Class 3. This causes the model to predict mostly Class 0, so the data needs to be balanced.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\columnwidth]{image/class_distribution.png}
\caption{Class distribution in training and test sets. Class 0 (Normal) dominates with 82.77\%, while minority classes (S, V, F) have very few samples.}
\label{fig:class_dist}
\end{figure}

\section{Methodology}

\subsection{Data Preprocessing}

\subsubsection{Data Type Conversion}
Features are changed to \texttt{float32} to save memory (uses 50\% less) and labels to \texttt{int64} for TensorFlow.

\subsubsection{SMOTE for Class Balancing}
\textbf{Why use SMOTE:} The dataset is very imbalanced. Without balancing, the model will just predict Class 0 (normal) for everything because it has the most samples. The model needs to learn all classes equally.

\textbf{How SMOTE works:} For each minority class sample $x_i$, SMOTE finds its $k$ nearest neighbors (k=5) and randomly selects one neighbor $x_{nn}$. A synthetic sample is created by linear interpolation:
\begin{equation}
x_{new} = x_i + \lambda \cdot (x_{nn} - x_i)
\end{equation}
where $\lambda \in [0,1]$ is a random value. This process repeats until all classes have equal representation. After SMOTE, all classes have 72,471 samples (362,355 total).

\subsubsection{Train-Validation Split}
The balanced data is split 80\% for training and 20\% for validation. Stratified sampling is used so each split has the same class distribution. The test set is kept imbalanced to see how the model works on real data.

\subsubsection{Data Reshaping}
Data is reshaped from (samples, 187) to (samples, 187, 1) to add a channel dimension for Conv1D layers.

\subsection{Model Architecture}

\textbf{Why 1D CNN:} A simple 1D CNN is used because:
\begin{itemize}
    \item It's easy to understand and train
    \item Works well for time-series signals like ECG
    \item Fast to train and run
\end{itemize}

\textbf{Convolutional Layer:} The 1D convolution operation extracts local features from the ECG signal. For input signal $x$ and filter $w$, the convolution output at position $t$ is:
\begin{equation}
y[t] = \sum_{i=0}^{k-1} w[i] \cdot x[t+i] + b
\end{equation}
where $k$ is the kernel size and $b$ is the bias term.

\textbf{Max Pooling:} Reduces spatial dimensions by taking the maximum value in each window:
\begin{equation}
\text{pool}(x) = \max_{i \in \text{window}} x[i]
\end{equation}

\textbf{Model structure:}
\begin{itemize}
    \item \textbf{Conv1D Layer 1:} 32 filters, kernel size 5, ReLU activation. Output shape: (187, 32)
    \item \textbf{MaxPooling1D:} Pool size 2. Output shape: (93, 32)
    \item \textbf{Conv1D Layer 2:} 64 filters, kernel size 5, ReLU activation. Output shape: (93, 64)
    \item \textbf{MaxPooling1D:} Pool size 2. Output shape: (46, 64)
    \item \textbf{Conv1D Layer 3:} 128 filters, kernel size 3, ReLU activation. Output shape: (46, 128)
    \item \textbf{GlobalMaxPooling1D:} Takes maximum across time dimension. Output shape: (128,)
    \item \textbf{Dense Layer:} 128 units with ReLU activation and Dropout (0.5)
    \item \textbf{Output Layer:} 5 units with softmax activation for class probabilities
\end{itemize}

The softmax function converts logits to probabilities:
\begin{equation}
P(y_i | x) = \frac{e^{z_i}}{\sum_{j=1}^{5} e^{z_j}}
\end{equation}
where $z_i$ is the logit for class $i$.

Total: 52,357 trainable parameters.

\subsection{Training Configuration}

\textbf{Optimizer:} Adam optimizer with learning rate $1 \times 10^{-3}$. Adam automatically adjusts learning rates and trains faster than SGD.

\textbf{Loss:} Sparse categorical crossentropy (labels are just numbers 0-4, not one-hot vectors).

\textbf{Training setup:}
\begin{itemize}
    \item Batch size: 128
    \item Epochs: 40 (stops early if no improvement for 8 epochs)
    \item Learning rate reduction: cuts learning rate in half when validation loss stops improving
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\columnwidth]{image/loss_and_Accuracy.png}
\caption{Training accuracy and loss curves over 40 epochs}
\label{fig:training_curves}
\end{figure}

\section{Results}

\subsection{Training Results}

The model trained for 36 epochs (early stopping). Final results:
\begin{itemize}
    \item Training accuracy: 99.12\%
    \item Validation accuracy: 99.36\%
\end{itemize}

Small gap between train/validation shows no overfitting.

\subsection{Test Results}

\textbf{Test accuracy: 94.64\%}

Performance by class:
\begin{itemize}
    \item Class 0 (Normal): Excellent (precision=0.99, recall=0.95, F1=0.97, support=18,118)
    \item Class 1 (Supraventricular): Low precision (0.35), high recall (0.83), F1=0.50, support=556 - model over-predicts this class
    \item Class 2 (Ventricular): Good (precision=0.93, recall=0.96, F1=0.94, support=1,448)
    \item Class 3 (Fusion): Moderate (precision=0.61, recall=0.89, F1=0.72, support=162) - confused with Class 2
    \item Class 4 (Unknown): Excellent (precision=0.99, recall=0.99, F1=0.99, support=1,608)
\end{itemize}

Figure~\ref{fig:confusion_matrix} shows the confusion matrix on the test set. The diagonal elements represent correct predictions. Most misclassifications occur between minority classes (S, V, F), which is expected given their similar characteristics and limited training samples. Class 0 (Normal) has the highest number of correct predictions (17,211 out of 18,118), while Class 1 (Supraventricular) has many false positives (461 correct out of 717 predicted), indicating the model tends to over-predict this class. Class 2 (Ventricular) performs well with 1,390 correct predictions out of 1,448. Class 3 (Fusion) has 144 correct predictions but is often confused with Class 2. Class 4 (Unknown) achieves near-perfect performance with 1,597 correct predictions out of 1,608.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\columnwidth]{image/confussion_matric.png}
\caption{Confusion matrix on test set. Diagonal elements show correct predictions. Most errors occur between minority classes (S, V, F).}
\label{fig:confusion_matrix}
\end{figure}

\subsection{Comparison with Original Paper}

The original paper by Kachuee et al. got 93.4\% accuracy on the same MIT-BIH dataset. They used a deeper CNN with residual blocks and transfer learning. Their model was more complex with skip connections.

This model got 94.64\% accuracy, which is 1.24\% better. There are a few reasons why this happened:

\textbf{SMOTE for balancing:} The original paper probably didn't use SMOTE. Since the dataset is very imbalanced (Class 0 has 113 times more samples than Class 3), their model would just predict Class 0 most of the time. Using SMOTE helps the model learn from all classes equally, so it performs better on minority classes.

\textbf{Better hyperparameters:} Different learning rates, batch sizes, and dropout rates were tested to find the best settings. The original paper might have used default values. The best settings found were learning rate $1 \times 10^{-3}$, batch size 128, and dropout 0.5.

\textbf{Adam optimizer:} Adam optimizer was used instead of older optimizers. Adam automatically adjusts the learning rate for each parameter, which makes training faster and more stable.

\textbf{Regularization:} Dropout 0.5 helps prevent overfitting. Early stopping and learning rate reduction also help the model generalize better to new data.

Even though the overall accuracy is better, the model still has trouble with minority classes (S and F). The original paper's deeper model with residual connections might be better at extracting features for these hard classes. To improve further, we could try:
\begin{itemize}
    \item Using class-weighted loss to focus more on minority classes
    \item Trying focal loss
    \item Combining multiple models (ensemble)
    \item Using a deeper model with residual connections like the original paper
\end{itemize}

\section{Conclusion}

A simple 1D CNN model was built for ECG heartbeat classification and got 94.64\% accuracy, which is better than the paper's 93.4\%. Using SMOTE to balance the dataset helped a lot, and the simple model works well when the data is preprocessed correctly. The model still needs improvement on minority classes (S and F), which could be done by using class-weighted loss or trying deeper architectures.

\end{multicols}

\end{document}